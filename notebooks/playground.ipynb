{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recognizer.utils.constants import ROOT_DIR, DATASET_DIR\n",
    "from recognizer.utils.utils import get_metadata_from_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "targets = []\n",
    "subjects = []\n",
    "repetitions = []\n",
    "files = []\n",
    "\n",
    "for file in os.listdir(DATASET_DIR):\n",
    "    if \"left\" in file:\n",
    "        continue\n",
    "\n",
    "    target, subject, repetition = get_metadata_from_filename(file)\n",
    "\n",
    "    targets.append(target)\n",
    "    subjects.append(subject)\n",
    "    repetitions.append(repetition)\n",
    "    files.append(str((DATASET_DIR / file).resolve()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>subject</th>\n",
       "      <th>repetition</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>064</td>\n",
       "      <td>002</td>\n",
       "      <td>001</td>\n",
       "      <td>/Users/facundopalavecino/Documents/DEV/ecd-tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>035</td>\n",
       "      <td>010</td>\n",
       "      <td>003</td>\n",
       "      <td>/Users/facundopalavecino/Documents/DEV/ecd-tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>033</td>\n",
       "      <td>009</td>\n",
       "      <td>003</td>\n",
       "      <td>/Users/facundopalavecino/Documents/DEV/ecd-tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>026</td>\n",
       "      <td>002</td>\n",
       "      <td>004</td>\n",
       "      <td>/Users/facundopalavecino/Documents/DEV/ecd-tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>023</td>\n",
       "      <td>005</td>\n",
       "      <td>002</td>\n",
       "      <td>/Users/facundopalavecino/Documents/DEV/ecd-tra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target subject repetition                                               file\n",
       "0    064     002        001  /Users/facundopalavecino/Documents/DEV/ecd-tra...\n",
       "1    035     010        003  /Users/facundopalavecino/Documents/DEV/ecd-tra...\n",
       "2    033     009        003  /Users/facundopalavecino/Documents/DEV/ecd-tra...\n",
       "3    026     002        004  /Users/facundopalavecino/Documents/DEV/ecd-tra...\n",
       "4    023     005        002  /Users/facundopalavecino/Documents/DEV/ecd-tra..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "metadata = pd.DataFrame(\n",
    "    data={\n",
    "        \"target\": targets,\n",
    "        \"subject\": subjects,\n",
    "        \"repetition\": repetitions,\n",
    "        \"file\": files,\n",
    "    }\n",
    ")\n",
    "\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recognizer.dataset import VideoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VideoDataset(video_filenames=files, labels=targets)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is composed by videos, we draw the first frame of each of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing some elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recognizer.utils.constants import LABELS_MAP\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "\n",
    "    sample_idx = torch.randint(len(dataset), size=(1,)).item()\n",
    "    img, label = dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(LABELS_MAP[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img[0].squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Pytorch documentation:\n",
    "\n",
    ">The Dataset retrieves our dataset’s features and labels one sample at a time. \n",
    "\n",
    "\n",
    ">While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell throws an error because pytorch is not able to stack all the elements accordingly. This is due to the elements having different size, because the videos have different amount of frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "# img = train_features[0][0].squeeze()\n",
    "# label = train_labels[0]\n",
    "# plt.imshow(img, cmap=\"gray\")\n",
    "# plt.show()\n",
    "# print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of frames\n",
    "\n",
    "Each video has different number of frames. We have to conciliate this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = []\n",
    "\n",
    "for element in dataset:\n",
    "    sizes.append(element[0].shape[0])\n",
    "\n",
    "sizes = pd.Series(data=sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    [sizes.describe(percentiles=[.1, .25, .5, .75, .9, .99])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like some videos only have **14** frames, whereas others have up to **201**. \n",
    "\n",
    "Somehow, we have to conciliate this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution #1: Sampling\n",
    "\n",
    "The goal is to return a fixed number of frames for each video, defined as `num_frames`.\n",
    "\n",
    "If the video has less frames than `num_frames`, then we loop over it until we have enough.\n",
    "If the video has more frames, then we substract `num_frames` equally spaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recognizer.dataset.sampled_video_dataset import SampledVideoDataset\n",
    "\n",
    "dataset = SampledVideoDataset(video_filenames=files, num_frames=25, labels=targets)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels}\")\n",
    "img = train_features[0][0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a hardware accelerator to train the model if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "Here I will code the 3D-CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recognizer.models.basic import BasicNeuralNetwork\n",
    "\n",
    "model = BasicNeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ea6ed51ea6f425be360bb9fdd5cef85dc80933cab3801b73250c70e567767ea"
  },
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit ('ecd-trabajo-final-UlVdN89U-py3.9': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
